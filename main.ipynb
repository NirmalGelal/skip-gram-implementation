{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"a quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(corpus):\n",
    "    corpus_lower = corpus.lower()\n",
    "    return (corpus_lower.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word_list):\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_data = encoder.fit_transform((np.array(word_list).reshape(-1,1)))\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "encoding = one_hot_encoding(preprocess_data(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product (vec1, vec2):\n",
    "    return sum(v1 * v2 for v1,v2 in zip(vec1, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(score, all_scores):\n",
    "    exp_score = [np.exp(score) for score in all_scores]\n",
    "    sum_exp_scores = sum(exp_score)\n",
    "    return np.exp(score)/sum_exp_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(probability, target, word_embedding, context_word_embedding):\n",
    "    gradient = (probability - target) * context_word_embedding\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_words(sentence, source_word_index, Window_size):\n",
    "    context_words = []\n",
    "    start = max(0, source_word_index - Window_size)\n",
    "    end = min(len(sentence), source_word_index + Window_size + 1)\n",
    "    for j in range(start, end):\n",
    "        if j != source_word_index:\n",
    "            context_words.append(sentence[j])\n",
    "    return context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(preprocess_data(corpus)) #length of vocabulary\n",
    "embedding_size = 5\n",
    "window_size = 2\n",
    "epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ramdonly initialize W and W_ between -0.5 and 0.5\n",
    "W = np.random.uniform(-0.5,0.5, (V, embedding_size))\n",
    "W_ = np.random.uniform(-0.5, 0.5, (V, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.35675099, -0.19165689, -0.04307702,  0.26098336,  0.35200362],\n",
       "        [-0.0842466 , -0.15733899,  0.4232296 ,  0.42572331,  0.06262268],\n",
       "        [ 0.22329653, -0.04691784, -0.00349711,  0.49174253, -0.18585359],\n",
       "        [ 0.20109137, -0.00946429,  0.37030383, -0.4542065 , -0.13572945],\n",
       "        [ 0.35492043,  0.08679703, -0.21207638,  0.25893624,  0.44234139],\n",
       "        [-0.25336049,  0.01079983,  0.48926434,  0.04995064,  0.35617572],\n",
       "        [-0.24701855, -0.46682775,  0.02733163,  0.46211667, -0.1452077 ],\n",
       "        [ 0.01328174, -0.27103376,  0.47498057,  0.16986278, -0.18414042],\n",
       "        [-0.2775434 ,  0.17604357,  0.28247009,  0.46732447, -0.34650421]]),\n",
       " array([[-0.15888281, -0.47322279,  0.39039164,  0.09378718,  0.20708696],\n",
       "        [ 0.50332802, -0.29785753, -0.17298933,  0.42819127, -0.27032192],\n",
       "        [-0.49078463, -0.04745013,  0.14573869,  0.4200493 ,  0.18152726],\n",
       "        [ 0.21978838,  0.13941535, -0.24852388, -0.09108882,  0.25747329],\n",
       "        [-0.04627517, -0.0970251 ,  0.24044547, -0.06531982, -0.10542521],\n",
       "        [-0.19521511, -0.27475867,  0.19133936, -0.27198873, -0.47316608],\n",
       "        [-0.38910272, -0.36557705, -0.37945986,  0.01645242,  0.22484134],\n",
       "        [ 0.14627599, -0.36906908, -0.31010461,  0.26092092, -0.07776718],\n",
       "        [ 0.02062901,  0.29506748, -0.21061595,  0.3169827 ,  0.35501187]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = preprocess_data(corpus)\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(word_list)):\n",
    "        source_word = word_list[i]\n",
    "        context_words = get_context_words(word_list, i, window_size)\n",
    "        source_word_embedding = W[word_list.index(source_word)]\n",
    "        all_scores = []\n",
    "        for word in word_list:\n",
    "            context_word_embedding = W_[word_list.index(word)]\n",
    "            score = dot_product(source_word_embedding, context_word_embedding)\n",
    "            all_scores.append(score)\n",
    "        for context_word in context_words:\n",
    "            context_word_embedding = W_[word_list.index(context_word)]\n",
    "            score = dot_product(source_word_embedding, context_word_embedding)\n",
    "            probability = softmax(score, all_scores)\n",
    "            target = 1 if context_word in context_words else 0\n",
    "            gradient_W = compute_gradient(probability, target, source_word_embedding, context_word_embedding)\n",
    "            gradient_W_ = compute_gradient(probability, target, context_word_embedding, source_word_embedding)\n",
    "            W[word_list.index(source_word)] = W[word_list.index(source_word)] - learning_rate * gradient_W\n",
    "            W_[word_list.index(context_word)] = W_[word_list.index(context_word)] - learning_rate * gradient_W_\n",
    "\n",
    "        \n",
    "W, W_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test (word, W, W_, word_list, window_size):\n",
    "    source_word_index = word_list.index(word)\n",
    "    source_word_embedding = W[source_word_index]\n",
    "    all_scores = []\n",
    "    for context_word_index, context_word in enumerate(word_list):\n",
    "        context_word_embedding = W_[context_word_index]\n",
    "        score = np.dot(source_word_embedding, context_word_embedding)\n",
    "        all_scores.append(score)\n",
    "    exp_scores = np.exp(all_scores)\n",
    "    sum_exp_scores = np.sum(exp_scores)\n",
    "    probabilities = exp_scores/sum_exp_scores\n",
    "    \n",
    "    predicted_targets = {}\n",
    "    for i,prob in enumerate(probabilities):\n",
    "        target_word = word_list[i]\n",
    "        if source_word_index - window_size <= i <= source_word_index + window_size and i != source_word_index:\n",
    "            predicted_targets[target_word] = prob\n",
    "    return predicted_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.10350231719615158,\n",
       " 'quick': 0.1536035133646259,\n",
       " 'fox': 0.0992224988029641,\n",
       " 'jumps': 0.10225346923393337}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test('brown', W, W_, word_list, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
